{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeAllan123/Dynamic-programing/blob/main/lusogatue_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_dGLEwQnaUl",
        "outputId": "959f0b2e-231e-4b96-f945-9d765e86c473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iwms32UoMAM",
        "outputId": "5c257190-d7bd-489d-87e7-4d0f53a847c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['en', 'soga'],\n",
            "        num_rows: 43893\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['en', 'soga'],\n",
            "        num_rows: 4877\n",
            "    })\n",
            "})\n",
            "{'en': 'Alcohol possesses a threat to the health of others.', 'soga': \"Omwenge gwa bulabe eri obulamu bw'abandi.\"}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Path to your cleaned CSV\n",
        "csv_path = '/content/drive/MyDrive/ML_projects/model1/soga_english.csv'\n",
        "\n",
        "# Load the CSV\n",
        "raw = load_dataset(\"csv\", data_files=csv_path)\n",
        "\n",
        "# Rename columns for clarity\n",
        "raw = raw[\"train\"].rename_columns({\"source\":\"en\",\"target\":\"soga\"})\n",
        "\n",
        "# Split into train/validation\n",
        "raw = raw.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# Check the splits\n",
        "print(raw)\n",
        "print(raw[\"train\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbQKCoU9DXwz",
        "outputId": "e3214327-ccea-409b-d5b5-3ffd9e0b338a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English→Swahili MarianMT loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# Model name for English → Swahili\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-swc\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Quick check\n",
        "print(\"English→Swahili MarianMT loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhgnOw0ODvyF",
        "outputId": "8018ad6c-008e-4a90-fad0-cad6d1ef0b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'en': 'Alcohol possesses a threat to the health of others.', 'soga': \"Omwenge gwa bulabe eri obulamu bw'abandi.\", 'input_ids': [14741, 19169, 13, 5392, 8, 5, 1013, 9, 274, 3, 0, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1134, 23316, 6267, 4246, 355, 15585, 1442, 2762, 55, 212, 1071, 55, 277, 3684, 1442, 1510, 8413, 3963, 22, 5671, 5626, 3, 0, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904, 58904]}\n"
          ]
        }
      ],
      "source": [
        "max_length = 128  # maximum sequence length\n",
        "\n",
        "def preprocess(batch):\n",
        "    # Ensure all entries are strings\n",
        "    inputs = [str(x) for x in batch[\"en\"]]\n",
        "    targets = [str(x) for x in batch[\"soga\"]]\n",
        "\n",
        "    # Tokenize inputs and targets\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Tokenize targets separately\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "    # Add labels to model inputs\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to train and validation sets\n",
        "tokenized_train = raw[\"train\"].map(preprocess, batched=True)\n",
        "tokenized_val   = raw[\"test\"].map(preprocess, batched=True)\n",
        "\n",
        "# Quick check\n",
        "print(tokenized_train[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDPYqnObKsDZ",
        "outputId": "2c5d5e29-1980-4898-a4e0-2151e42d9bb9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5Uwd9dbEB9W",
        "outputId": "2d4b6a30-3d89-4f19-eef4-9d77e889337c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer is ready!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-114741585.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# Data collator for dynamic padding\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Load BLEU metric for evaluation\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 in labels as padding token\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Compute BLEU score\n",
        "    result = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
        "    return {\"bleu\": result[\"score\"]}\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./marian-soga-out\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    eval_accumulation_steps=2,\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=5e-5,\n",
        "    logging_dir=\"./logs\",\n",
        "    fp16=True  # mixed precision for faster training\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer is ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "mnMQE1-EFjhO",
        "outputId": "fe3242b9-b866-48cd-e16b-70289b21bc9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmeregulwaallan7\u001b[0m (\u001b[33mmeregulwaallan7-soroti-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "creating run (0.0s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250909_224100-9yha801z</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/meregulwaallan7-soroti-university/huggingface/runs/9yha801z' target=\"_blank\">noble-glitter-6</a></strong> to <a href='https://wandb.ai/meregulwaallan7-soroti-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/meregulwaallan7-soroti-university/huggingface' target=\"_blank\">https://wandb.ai/meregulwaallan7-soroti-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/meregulwaallan7-soroti-university/huggingface/runs/9yha801z' target=\"_blank\">https://wandb.ai/meregulwaallan7-soroti-university/huggingface/runs/9yha801z</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='23' max='13720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   23/13720 08:33 < 92:59:37, 0.04 it/s, Epoch 0.01/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNZY7NpHv20HuyANN2kD1mA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}